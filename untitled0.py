# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LMHh5zEJUN7qr7Fb-AgfYkuNBSVjaT5C
"""

pip install transformers datasets peft accelerate torch bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM,AutoModelForQuestionAnswering
from datasets import load_dataset
import torch
import bitsandbytes as bnb

# Set your Hugging Face API token
hf_access_token = "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"

# Load the tokenizer with access token

tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")
model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2",
                                                      load_in_4bit=True,  # Enable 4-bit quantization
    torch_dtype=torch.float16,  # Set the computation data type
    device_map='auto',  # Automatically map model to available devices
    use_auth_token=hf_access_token)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Prepare model for 4-bit training
model = prepare_model_for_kbit_training(model)

# Define LoRA configuration
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="QA"  # Task type is question-answering
)

# Add LoRA adapters to the model
model = get_peft_model(model, lora_config)

class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # Forward pass
        outputs = model(**inputs)
        start_logits, end_logits = outputs.start_logits, outputs.end_logits

        # Compute the loss
        start_positions = inputs["start_positions"]
        end_positions = inputs["end_positions"]

        # Ensure positions are not out of range
        ignored_index = start_logits.size(1)
        start_positions = start_positions.clamp(0, ignored_index)
        end_positions = end_positions.clamp(0, ignored_index)

        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)
        start_loss = loss_fct(start_logits, start_positions)
        end_loss = loss_fct(end_logits, end_positions)
        total_loss = (start_loss + end_loss) / 2

        return (total_loss, outputs) if return_outputs else total_loss

# Load the dataset with access token
dataset = load_dataset('squad', split='train[:1%]', use_auth_token=hf_access_token)  # Use only 1% of the training set
dataset = dataset.train_test_split(test_size=0.1)  # Split off a small test set

# Preprocess the dataset
def preprocess_function(examples):
    inputs = tokenizer(
        examples["question"], examples["context"],
        truncation=True, padding="max_length", max_length=384
    )
    inputs["start_positions"] = [ans["answer_start"][0] for ans in examples["answers"]]
    inputs["end_positions"] = [ans["answer_start"][0] + len(ans["text"][0]) for ans in examples["answers"]]
    return inputs

tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
    Trainer
)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=1,  # Adjust based on your GPU's memory
    logging_dir='./logs',
    save_steps=1000,
    save_total_limit=2,
)

# Create the trainer
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
)


# Fine-tune the model
trainer.train()

def answer_question(question, context):
    # Tokenize input question and context
    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True, max_length=384)



    # Get model predictions
    with torch.no_grad():
        outputs = model(**inputs)
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

    # Get the most likely beginning and end of answer span
    start_idx = torch.argmax(start_logits)
    end_idx = torch.argmax(end_logits) + 1

    # Convert token indices to actual answer text
    input_ids = inputs["input_ids"].cpu().numpy()[0]
    answer_tokens = input_ids[start_idx:end_idx]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    return answer

# Example usage
question = "Who is alex?"
context = "alex is capital city is Paris, which is known for its art, culture, and landmarks like the Eiffel Tower."

answer = answer_question(question, context)
print(f"Question: {question}")
print(f"Context: {context}")
print(f"Answer: {answer}")